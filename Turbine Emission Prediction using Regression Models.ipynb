{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5092b3e3-30b8-44ca-8e1b-8b499aaf5eee",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d60e3631-1029-47e1-bb49-790227bd8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (7384, 11)\n",
      "\n",
      "First 5 rows:\n",
      "         AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
      "0  1.95320  1020.1  84.985  2.5304  20.116  1048.7  544.92  116.27  10.799   \n",
      "1  1.21910  1020.1  87.523  2.3937  18.584  1045.5  548.50  109.18  10.347   \n",
      "2  0.94915  1022.2  78.335  2.7789  22.264  1068.8  549.95  125.88  11.256   \n",
      "3  1.00750  1021.7  76.942  2.8170  23.358  1075.2  549.63  132.21  11.702   \n",
      "4  1.28580  1021.6  76.732  2.8377  23.483  1076.2  549.68  133.58  11.737   \n",
      "\n",
      "       CO      NOX  \n",
      "0  7.4491  113.250  \n",
      "1  6.4684  112.020  \n",
      "2  3.6335   88.147  \n",
      "3  3.1972   87.078  \n",
      "4  2.3833   82.515  \n",
      "\n",
      "Data Types:\n",
      " AT      float64\n",
      "AP      float64\n",
      "AH      float64\n",
      "AFDP    float64\n",
      "GTEP    float64\n",
      "TIT     float64\n",
      "TAT     float64\n",
      "TEY     float64\n",
      "CDP     float64\n",
      "CO      float64\n",
      "NOX     float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      " AT      0\n",
      "AP      0\n",
      "AH      0\n",
      "AFDP    0\n",
      "GTEP    0\n",
      "TIT     0\n",
      "TAT     0\n",
      "TEY     0\n",
      "CDP     0\n",
      "CO      0\n",
      "NOX     0\n",
      "dtype: int64\n",
      "\n",
      "Ridge Regression Results:\n",
      "R² Score: 0.8353\n",
      "MSE: 10.5511\n",
      "MAE: 2.5526\n",
      "RMSE: 3.2482\n",
      "Best Alpha: 0.0010\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Load and explore data\n",
    "data = pd.read_csv('gt_2015.csv')\n",
    "\n",
    "# Basic EDA\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"\\nFirst 5 rows:\\n\", data.head())\n",
    "print(\"\\nData Types:\\n\", data.dtypes)\n",
    "print(\"\\nMissing Values:\\n\", data.isnull().sum())\n",
    "\n",
    "#  Data Cleaning\n",
    "# Handle missing values\n",
    "if data.isnull().any().any():\n",
    "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Outlier treatment using IQR\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    data = data[(data[col] >= Q1 - 1.5*IQR) & (data[col] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# 3. Feature Engineering\n",
    "X = data.drop(columns=['NOX'])\n",
    "y = data['NOX']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform target variable\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "y_train_trans = pt.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_trans = pt.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_train_selected = selector.fit_transform(X_train_poly, y_train_trans)\n",
    "X_test_selected = selector.transform(X_test_poly)\n",
    "\n",
    "# 4. Model Optimization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Ridge regression with hyperparameter tuning\n",
    "ridge_params = {'alpha': np.logspace(-3, 3, 20)}\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='r2')\n",
    "ridge_grid.fit(X_train_scaled, y_train_trans)\n",
    "\n",
    "# Make predictions\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "y_pred_ridge = pt.inverse_transform(best_ridge.predict(X_test_scaled).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_test, y_pred_ridge)\n",
    "mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\nRidge Regression Results:\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Best Alpha: {ridge_grid.best_params_['alpha']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d98d6e-b8ce-4368-9c82-eadfacf413c4",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab79589e-4177-4e06-9df2-c83aca38098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regression Results:\n",
      "Best Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "R² Score: 0.7587\n",
      "MSE: 15.4632\n",
      "RMSE: 3.9323\n",
      "MAE: 2.9716\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Load and explore data\n",
    "data = pd.read_csv('gt_2015.csv')\n",
    "\n",
    "# 2. Data Cleaning\n",
    "# Handle missing values\n",
    "if data.isnull().any().any():\n",
    "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Outlier treatment using IQR\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    data = data[(data[col] >= Q1 - 1.5*IQR) & (data[col] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# 3. Feature Engineering\n",
    "X = data.drop(columns=['NOX'])\n",
    "y = data['NOX']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform skewed target variable\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "y_train_trans = pt.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_trans = pt.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_train_selected = selector.fit_transform(X_train_poly, y_train_trans)\n",
    "X_test_selected = selector.transform(X_test_poly)\n",
    "\n",
    "# 4. Decision Tree Model with Hyperparameter Tuning\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize GridSearch\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=dt_reg, param_grid=param_grid, \n",
    "                          cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train_selected, y_train_trans)\n",
    "\n",
    "# Best model evaluation\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred_trans = best_dt.predict(X_test_selected)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nDecision Tree Regression Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00254f77-8eaa-4f46-a169-1f26e45b3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "Best Parameters: {'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 400}\n",
      "R² Score: 0.8688\n",
      "MSE: 8.4047\n",
      "RMSE: 2.8991\n",
      "MAE: 2.1587\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "data = pd.read_csv('gt_2015.csv')\n",
    "\n",
    "# Data cleaning\n",
    "if data.isnull().any().any():\n",
    "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    data = data[(data[col] >= Q1 - 1.5*IQR) & (data[col] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# Feature engineering\n",
    "X = data.drop(columns=['NOX'])\n",
    "y = data['NOX']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Advanced feature engineering\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=30)\n",
    "X_train_selected = selector.fit_transform(X_train_poly, y_train)\n",
    "X_test_selected = selector.transform(X_test_poly)\n",
    "\n",
    "# 2. Hyperparameter tuning with Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 400],\n",
    "    'max_depth': [20, 30],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [False]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, \n",
    "                         scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317ffbf-04c4-4fc4-8e4b-5e05064789c0",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95e4cac7-605c-47d0-94c5-8e351b334240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 8.503843596829473\n",
      "Mean Absolute Error: 1.8997867661823016\n",
      "RMSE: 2.916135044340278\n",
      "R-squared: 0.9333157447350111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('gt_2015.csv')\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "svr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "y_pred_scaled = svr.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_test_original = y_test\n",
    "\n",
    "mse = mean_squared_error(y_test_original, y_pred)\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_original, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f'R-squared: {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
